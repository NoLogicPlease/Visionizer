{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cffg2i257iMS"
      },
      "source": [
        "# Visionizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyqE1M0iA4I_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"We're running Colab\")\n",
        "    # Mount the Google Drive at mount\n",
        "    mount = '/content/gdrive'\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Switch to the directory on the Google Drive that you want to use\n",
        "    drive_root = mount + \"/My Drive/NLP/Visionizer/\"\n",
        "\n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "        print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "        os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    % cd $drive_root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8l4RJ0XRPEm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if IN_COLAB:\n",
        "    !pip install bert-score\n",
        "    !pip install matplotlib==3.4\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim.downloader as gloader\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import h5py\n",
        "import copy\n",
        "import time\n",
        "import re\n",
        "import io\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import single_meteor_score, meteor_score\n",
        "from bert_score import score as bert_score\n",
        "from nltk.translate.chrf_score import sentence_chrf\n",
        "from operator import itemgetter\n",
        "from tqdm.auto import tqdm\n",
        "from numba import cuda\n",
        "from PIL import Image\n",
        "\n",
        "tf.get_logger().setLevel('WARNING')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T9IXF2CqGFf"
      },
      "source": [
        "If running in Google Colab, restart runtime after running the previous cell to update libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hMOasybAOau"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 2\n",
        "BASELINE = False\n",
        "LSTM = True\n",
        "DATASET_NAME = \"FACAD\"\n",
        "BATCH_SIZE = 64\n",
        "EMBEDDING_SIZE = 300\n",
        "VAL_TEST_PER = 0.2\n",
        "units = 512\n",
        "# 2 are extra for the start and end tokens\n",
        "MAX_SEQ_LEN = 22 \n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "assert DATASET_NAME in [\"FashionGen\", \"InFashAI_DeepFashion\", \"FACAD\"]\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(f\"GPUs: {gpus}\")\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    \n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9xmsEQYZmDh"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cER3BasZItYS"
      },
      "source": [
        "## Load Dataset and Clean Captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krQuPYTtRPE7"
      },
      "outputs": [],
      "source": [
        "s_lens = []\n",
        "c_lens = []\n",
        "\n",
        "def clean_captions(sentences, fashiongen=False):\n",
        "    cleaned = []\n",
        "    for s in tqdm(sentences):\n",
        "        sen = s.decode('ISO-8859-1').lower()\n",
        "        sen = re.sub(r'\\.', ' ,', sen)\n",
        "        sen = re.sub('<br>', ' ', sen)\n",
        "        sen = re.sub(',(\\w)', ', \\1', sen)\n",
        "        sen = re.sub('(\\w),', '\\1 ,', sen)\n",
        "        sen = re.sub(r'[^\\w\\s\\,-<>]', '', sen)\n",
        "        sen = re.sub('-', ' ', sen)\n",
        "        if fashiongen:\n",
        "            # consider only first comma comment\n",
        "            sen = re.sub(r',.+$', '', sen)\n",
        "        sen = re.sub(',', '', sen)\n",
        "        sen = '<start> ' + sen + ' <end>'\n",
        "        sen = re.sub(' +', ' ', sen)\n",
        "        sen = sen.lower()\n",
        "        cleaned.append(sen)\n",
        "        s_lens.append(len(sen.split()[1:-1])) # [1:-1] to not consider start end \n",
        "        c_lens.append(len(sen) - 14) # 14 to not consider start end \n",
        "    return cleaned\n",
        "\n",
        "dataset = h5py.File(f\"{DATASET_NAME}/dataset/{DATASET_NAME}.h5\", 'r')\n",
        "if DATASET_NAME == \"FashionGen\":\n",
        "    cleaned_captions = clean_captions(dataset['input_description'], fashiongen=True)\n",
        "elif DATASET_NAME in [\"InFashAI_DeepFashion\", \"FACAD\"]:\n",
        "    cleaned_captions = clean_captions(dataset['input_description'])\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "    \n",
        "plt.boxplot(s_lens)  # looking for outliers in sentence length\n",
        "plt.show()\n",
        "\n",
        "img_indexes = list(range(len(cleaned_captions)))\n",
        "if max(s_lens) < MAX_SEQ_LEN:\n",
        "    MAX_SEQ_LEN = max(s_lens)\n",
        "\n",
        "print()\n",
        "print(f\"Lengths before start end [mean, min, max, median]:\\n \"\n",
        "      f\"{np.mean(s_lens):.3F}\\t {min(s_lens)}\\t {max(s_lens)}\\t {np.median(s_lens):.3F} \")\n",
        "print(f\"Characters before start end [mean, min, max, median]:\\n \"\n",
        "      f\"{np.mean(c_lens):.3F}\\t {min(c_lens)}\\t {max(c_lens)}\\t {np.median(c_lens):.3F} \")\n",
        "    \n",
        "# not including sentences longer than the MAX_SEQ_LEN\n",
        "img_indexes_maxthr = []\n",
        "cleaned_captions_maxthr = []\n",
        "\n",
        "for i, c in enumerate(cleaned_captions):\n",
        "    if len(c.split()) <= MAX_SEQ_LEN: \n",
        "        img_indexes_maxthr.append(img_indexes[i])\n",
        "        cleaned_captions_maxthr.append(c)\n",
        "        \n",
        "print(\"Removed: \", len(img_indexes) - len(img_indexes_maxthr), \" items\")\n",
        "print(len(img_indexes_maxthr), len(cleaned_captions_maxthr))\n",
        "print()\n",
        "\n",
        "s_lens = [len(s.split()[1:-1]) for s in cleaned_captions_maxthr]\n",
        "c_lens = [len(s) - 14 for s in cleaned_captions_maxthr]\n",
        "print(f\"Lengths after max threshold [mean, min, max, median]:\\n \"\n",
        "      f\"{np.mean(s_lens):.3F}\\t {min(s_lens)}\\t {max(s_lens)}\\t {np.median(s_lens):.3F} \")\n",
        "print(f\"Characters after max threshold [mean, min, max, median]:\\n \"\n",
        "      f\"{np.mean(c_lens):.3F}\\t {min(c_lens)}\\t {max(c_lens)}\\t {np.median(c_lens):.3F} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3R9rT9MiGUc"
      },
      "outputs": [],
      "source": [
        "# Copy to COLAB local disk to have faster access\n",
        "if IN_COLAB:\n",
        "    data_path = f\"{DATASET_NAME}/dataset/{DATASET_NAME}.h5\"\n",
        "    !cp -v $data_path /content/\n",
        "    dataset = h5py.File(f\"{DATASET_NAME}/dataset/{DATASET_NAME}.h5\", 'r')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrmdtMX8Lnyh"
      },
      "outputs": [],
      "source": [
        "for i in range(1):\n",
        "    idx = np.random.randint(len(dataset['input_image']))\n",
        "    img = tf.io.decode_jpeg(dataset['input_image'][idx]).numpy()\n",
        "    plt.matshow(img)\n",
        "    print(f\"{idx}, Shape: {img.shape}, {cleaned_captions[idx]}\")\n",
        "\n",
        "del idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXEfxZ96cd1W"
      },
      "source": [
        "## Features Extraction: InceptionV3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4qFOVTeJnYU"
      },
      "source": [
        "### Load InceptionV3 pretrained weights\n",
        "\n",
        "In order to extract features we use InceptionV3 (which is pretrained on Imagenet). Features are extracted from the last convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhCND0bCUP11"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cSW4u-ORPFQ"
      },
      "source": [
        "### Preprocess the images using InceptionV3\n",
        "\n",
        "Before processing images with InceptionV3, we convert them into InceptionV3's expected format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXR0217aRPFR"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def preprocess_image(img):\n",
        "    # resize all the images in the dataset before for faster generation of batches\n",
        "    # otherwise uncomment the following\n",
        "    # img = tf.keras.layers.experimental.preprocessing.Resizing(299, 299)(img)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "@tf.function\n",
        "def forward_in_inception(images):\n",
        "    batch = tf.map_fn(preprocess_image, images)\n",
        "    batch_features = image_features_extract_model(batch)\n",
        "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "    return batch_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMvwuX4hd5uz"
      },
      "source": [
        "## Caption Processing\n",
        "\n",
        "Once the images are prepared, we have to tokenize the captions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDeUcnUKrh0l"
      },
      "source": [
        "### Tokenizer (Class)\n",
        "\n",
        "Tokenizer is the class implemented to deal with text tokenization. It creates the embedding matrix and the dictionaries used to map words with tokens and viceversa. \n",
        "The embedding used are GloVe embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-tngoYFrkyz"
      },
      "outputs": [],
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self, dataset_sentences, embedding_dim, glove_dict, glove_matrix):\n",
        "        \"\"\"\n",
        "            dataset_sentences : sentences to be tokenized in form of strings\n",
        "            embedding_dim : size of the embeddings\n",
        "            glove_dict : glove dictionary\n",
        "            glove_matrix : glove matrix\n",
        "        \"\"\"\n",
        "        self.embedding_matrix = None\n",
        "        # word to token dictionary\n",
        "        self.value_to_key = {}\n",
        "        # useful for implementation, contains only new OOV terms\n",
        "        self.value_to_key_new = {}\n",
        "        self.num_unique_words = 0\n",
        "        # word to token\n",
        "        self.key_to_value = {}\n",
        "        self.dataset_sentences = dataset_sentences\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.glove_dict = glove_dict\n",
        "        self.glove_matrix = glove_matrix\n",
        "        # set containing all the words of the vocabulary, update after the processing of each split\n",
        "        self.unique_words = set()\n",
        "\n",
        "    def get_val_to_key(self):\n",
        "        return copy.deepcopy(self.value_to_key)\n",
        "\n",
        "    def tokenize(self):\n",
        "        \"\"\"\n",
        "            create and/or upload tokenizer. Each time that a new word is passed\n",
        "            from a sentence, check if it has already been tokenized otherwise \n",
        "            tokenize it and add to the vocabulary. \n",
        "        \"\"\"\n",
        "        self.value_to_key_new = {}\n",
        "        unique_words = set()\n",
        "        for sen in self.dataset_sentences:\n",
        "            for w in sen.split():\n",
        "                # get set of unique words\n",
        "                unique_words.add(w)\n",
        "                # new unique are the words not still processed\n",
        "        new_unique = unique_words - self.unique_words\n",
        "        for i, word in enumerate(new_unique):\n",
        "            # it means we are in the validation or test set, the embedding has already been created and updated by the train\n",
        "            if self.embedding_matrix is not None:\n",
        "                # tokenization\n",
        "                self.key_to_value[i + len(self.embedding_matrix)] = word\n",
        "                self.value_to_key[word] = i + len(self.embedding_matrix)\n",
        "            else:\n",
        "                # first time we are tokenizing (train), don't need to add len\n",
        "                self.key_to_value[i] = word\n",
        "                self.value_to_key[word] = i\n",
        "            self.value_to_key_new[word] = i\n",
        "\n",
        "        self.num_unique_words = len(new_unique)\n",
        "        # update unique words with new unique\n",
        "        self.unique_words = self.unique_words | new_unique\n",
        "\n",
        "    def __build_embedding_matrix_glove(self):\n",
        "        \"\"\"\n",
        "            create the embedding matrix. The rows corresponding to the words\n",
        "            contained in glove will be filled. If a word is not in glove the\n",
        "            word and its index are saved in order to be processed later.   \n",
        "        \"\"\"\n",
        "        oov_words = []\n",
        "        tmp_embedding_matrix = np.zeros((self.num_unique_words, self.embedding_dim))\n",
        "        len_old_emb_matrix = len(self.embedding_matrix) if self.embedding_matrix is not None else 0\n",
        "        for word, idx in tqdm(self.value_to_key_new.items()):\n",
        "            try:\n",
        "                embedding_vector = self.glove_matrix[self.glove_dict[word]]\n",
        "                # create tmp embedding matrix to be concatenated to the original embedding matrix\n",
        "                tmp_embedding_matrix[idx] = embedding_vector\n",
        "            except (KeyError, TypeError):\n",
        "                oov_words.append((word, idx + len_old_emb_matrix))\n",
        "\n",
        "        if self.embedding_matrix is not None:\n",
        "            # concatenate old embedding matrix and new one (new OOVs)\n",
        "            self.embedding_matrix = np.vstack((self.embedding_matrix, tmp_embedding_matrix))\n",
        "        else:\n",
        "            self.embedding_matrix = copy.deepcopy(tmp_embedding_matrix)\n",
        "        return oov_words\n",
        "\n",
        "    def build_embedding_matrix(self):\n",
        "        \"\"\"\n",
        "            create embedding vector for OOV words. If a word \n",
        "            is out of vocabulary values follow uniform distribution . \n",
        "        \"\"\"\n",
        "        oov_words = self.__build_embedding_matrix_glove()\n",
        "        for word, idx in oov_words:\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=self.embedding_dim)\n",
        "            self.embedding_matrix[idx] = embedding_vector\n",
        "        return copy.deepcopy(self.embedding_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyqH3zFwRPFi"
      },
      "source": [
        "### Preprocess and tokenize the captions\n",
        "\n",
        "Transform the text captions into integer sequences using the Tokenizer.\n",
        "We remove elements in the datset bigger than  a threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FegYXpxIhcPv"
      },
      "source": [
        "In the following code we first load Glove Embeddings. After that we use the Tokenizer methods to tokenize the text (map each word to an integer, the token) and we create the embedding matrix. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJGE34aiRPFo"
      },
      "outputs": [],
      "source": [
        "# Load Glove Embeddings from a pickle file or download if necessary\n",
        "try:\n",
        "    with open(f\"./GloVe/glove-{EMBEDDING_SIZE}.pkl\", 'rb') as f:\n",
        "        emb_model = pickle.load(f)\n",
        "except Exception:\n",
        "    emb_model = gloader.load(f\"glove-wiki-gigaword-{EMBEDDING_SIZE}\")\n",
        "    with open(f\"./GloVe/glove-{EMBEDDING_SIZE}.pkl\", 'wb') as f:\n",
        "        pickle.dump(emb_model, f)\n",
        "####################################################################\n",
        "\n",
        "glove_dict = emb_model.key_to_index\n",
        "glove_matrix = emb_model.vectors\n",
        "\n",
        "tokenizer = Tokenizer(cleaned_captions_maxthr, EMBEDDING_SIZE, glove_dict, glove_matrix)\n",
        "tokenizer.tokenize()\n",
        "\n",
        "# create embedding matrix\n",
        "emb_matrix = tokenizer.build_embedding_matrix()\n",
        "# first column corresponding to padding\n",
        "emb_matrix = np.vstack((np.zeros((1, EMBEDDING_SIZE)), emb_matrix))\n",
        "\n",
        "val_to_key = tokenizer.get_val_to_key()\n",
        "\n",
        "# Shifts tokens by 1 because of padding (to adapt to the network embedding layer)\n",
        "val_to_key.update((x, y + 1) for x, y in val_to_key.items())\n",
        "\n",
        "# Translation dictionary to retranslate from tokens to words\n",
        "key_to_val = {}\n",
        "key_val_list_items = list(tokenizer.key_to_value.items())\n",
        "for i, (token, value) in enumerate(key_val_list_items):\n",
        "    if i > 0:\n",
        "        key_to_val[token] = key_val_list_items[i - 1][1]\n",
        "    else:\n",
        "        key_to_val[i] = '<PAD>'\n",
        "\n",
        "key_to_val[len(key_val_list_items)] = key_val_list_items[-1][1]\n",
        "\n",
        "\n",
        "def word_to_index(word):\n",
        "    return val_to_key[word]\n",
        "\n",
        "\n",
        "def index_to_word(index):\n",
        "    return key_to_val[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uaq07VVEu36f"
      },
      "outputs": [],
      "source": [
        "# Create the tokenized vectors\n",
        "cap_vector = []\n",
        "for sen in cleaned_captions_maxthr:\n",
        "    cap_vector.append([word_to_index(i) for i in sen.split()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI-VQxhpjzDI"
      },
      "source": [
        "## Training Preparation and Model Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3CD75nDpvTI"
      },
      "source": [
        "### Split the data into training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS7DDMszRPGF"
      },
      "outputs": [],
      "source": [
        "img_to_cap_vector = {}\n",
        "#for img, cap in zip(img_name_vector, cap_vector):\n",
        "for img, cap in zip(img_indexes_maxthr, cap_vector):\n",
        "    img_to_cap_vector[img] = cap\n",
        "# Create training and validation sets using an 80-20 split randomly.\n",
        "img_keys = list(img_to_cap_vector.keys())\n",
        "\n",
        "# It is important not to shuffle the FashionGen dataset all together\n",
        "# since there are multiple images per caption in order\n",
        "if DATASET_NAME != \"FashionGen\":\n",
        "    random.shuffle(img_keys)\n",
        "print(len(img_keys))\n",
        "\n",
        "slice_index = int(len(img_keys) * (1 - VAL_TEST_PER))\n",
        "slice_index_test = int(len(img_keys) * (1 - (VAL_TEST_PER/2)))\n",
        "\n",
        "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:slice_index_test]\n",
        "img_name_test_keys = img_keys[slice_index_test:]\n",
        "\n",
        "# TRAIN SPLIT\n",
        "img_name_train = []\n",
        "cap_train = []\n",
        "for img in img_name_train_keys:\n",
        "    img_name_train.append(img)\n",
        "    img_to_cap_vector[img] = img_to_cap_vector[img] + ([0] * (MAX_SEQ_LEN - len(img_to_cap_vector[img])))\n",
        "    cap_train.append(img_to_cap_vector[img])\n",
        "# Shuffle in split\n",
        "train_pairs = list(zip(img_name_train, cap_train))\n",
        "random.shuffle(train_pairs)\n",
        "img_name_train, cap_train = zip(*train_pairs)\n",
        "del train_pairs\n",
        "\n",
        "# VAL SPLIT\n",
        "img_name_val = []\n",
        "cap_val = []\n",
        "for img in img_name_val_keys:\n",
        "    img_name_val.append(img)\n",
        "    img_to_cap_vector[img] = img_to_cap_vector[img] + ([0] * (MAX_SEQ_LEN - len(img_to_cap_vector[img])))\n",
        "    cap_val.append(img_to_cap_vector[img])\n",
        "# Shuffle in split\n",
        "val_pairs = list(zip(img_name_val, cap_val))\n",
        "random.shuffle(val_pairs)\n",
        "img_name_val, cap_val = zip(*val_pairs)\n",
        "del val_pairs\n",
        "\n",
        "\n",
        "# TEST SPLIT\n",
        "img_name_test = []\n",
        "cap_test = []\n",
        "for img in img_name_test_keys:\n",
        "    img_name_test.append(img)\n",
        "    img_to_cap_vector[img] = img_to_cap_vector[img] + ([0] * (MAX_SEQ_LEN - len(img_to_cap_vector[img])))\n",
        "    cap_test.append(img_to_cap_vector[img])\n",
        "# Shuffle in split\n",
        "test_pairs = list(zip(img_name_test, cap_test))\n",
        "random.shuffle(test_pairs)\n",
        "img_name_test, cap_test = zip(*test_pairs)\n",
        "del test_pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmViPkRFRPGH"
      },
      "outputs": [],
      "source": [
        "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val), len(img_name_test), len(cap_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3TnZ1ToRPGV"
      },
      "outputs": [],
      "source": [
        "num_steps_per_epoch = len(img_name_train) // BATCH_SIZE\n",
        "num_steps_val_per_epoch = len(img_name_val) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64\n",
        "\n",
        "print(f\"Train Steps: {num_steps_per_epoch} | Validation Steps: {num_steps_val_per_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anmiMJTapbex"
      },
      "outputs": [],
      "source": [
        "print('first sentence check: \\n', [index_to_word(i) for i in cap_test[0]])\n",
        "print('test shape: ', len(cap_test))\n",
        "plt.matshow(tf.io.decode_jpeg(dataset['input_image'][img_name_test[0]]).numpy())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTwv-U2IAOa3"
      },
      "outputs": [],
      "source": [
        "# Yield also the following to debug the order of the indexes:\n",
        "# indexes = [el for el in img_name_train[i * bs : (i * bs) + bs]]\n",
        "\n",
        "def dataset_generator(img_names, captions):\n",
        "    bs = BATCH_SIZE\n",
        "    i = 0\n",
        "    while i < len(captions) and (len(captions[i * bs : -1]) + 1 >= bs):\n",
        "        images_in_batch = np.array([tf.io.decode_jpeg(dataset[\"input_image\"][el], fancy_upscaling=False)\n",
        "                                    for el in img_names[i * bs : (i * bs) + bs]], dtype=np.float32)\n",
        "        features_in_batch = forward_in_inception(images_in_batch)\n",
        "        captions_in_batch = np.array(captions[i * bs : (i * bs) + bs], dtype=np.int32)\n",
        "        i+=1\n",
        "        yield features_in_batch, captions_in_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYM8PyTbAOa3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for (batch, (img_tensor, target)) in enumerate(dataset_generator(img_name_train, cap_train)):\n",
        "    # Some Debugging\n",
        "    # print(type(batch), batch)\n",
        "    # print(type(img_tensor), img_tensor.shape)\n",
        "    # print(len(target))\n",
        "    # print(' '.join([index_to_word(w) for w in target[0]]))\n",
        "    if batch == 0:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrvoDphgRPGd"
      },
      "source": [
        "### Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja2LFTMSdeV3"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.Module):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 64, 300], dtype=tf.float32),\n",
        "                                  tf.TensorSpec(shape=[None, 512], dtype=tf.float32)])\n",
        "    def __call__(self, features, hidden):\n",
        "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "        # hidden shape == (batch_size, hidden_size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
        "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
        "                                             self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # score shape == (batch_size, 64, 1)\n",
        "        # This gives you an unnormalized score for each image feature.\n",
        "        score = self.V(attention_hidden_layer)\n",
        "\n",
        "        # attention_weights shape == (batch_size, 64, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4ZSl6RAAOa3"
      },
      "outputs": [],
      "source": [
        "class NoAttention(tf.Module):\n",
        "    def __init__(self, ):\n",
        "        super(NoAttention, self).__init__()\n",
        "        \n",
        "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 64, 300], dtype=tf.float32)])\n",
        "    def __call__(self, features):\n",
        "        context_vector = features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ7R1RxHRPGf"
      },
      "outputs": [],
      "source": [
        "class CNN_Encoder(tf.Module):\n",
        "    # Since you have already extracted the features and dumped it\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "    \n",
        "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, attention_features_shape, features_shape], dtype=tf.float32),]) \n",
        "    def __call__(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9hpRpE6HOoR"
      },
      "outputs": [],
      "source": [
        "class RNN_Decoder(tf.Module):\n",
        "    def __init__(self, units, embedding_matrix, vocab_size, lstm_enabled=False, baseline=False):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "        self.lstm_enabled = lstm_enabled\n",
        "        self.baseline = baseline\n",
        "        if self.baseline:            \n",
        "            self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                                   output_dim=EMBEDDING_SIZE,\n",
        "                                                   input_length=MAX_SEQ_LEN,\n",
        "                                                   mask_zero=True,\n",
        "                                                   trainable=True\n",
        "                                                   )\n",
        "        else:\n",
        "            self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                                    output_dim=EMBEDDING_SIZE,\n",
        "                                                    input_length=MAX_SEQ_LEN,\n",
        "                                                    mask_zero=True,\n",
        "                                                    weights=tf.expand_dims(embedding_matrix, axis=0),\n",
        "                                                    trainable=True\n",
        "                                                    )\n",
        "        self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.lstm = tf.keras.layers.LSTM(self.units,\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True,\n",
        "                                         recurrent_initializer='glorot_uniform')\n",
        "\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "        self.no_attention = NoAttention()\n",
        "        \n",
        "    # Credits to MariaMsu and ant0nisk for explaining how to save\n",
        "    # Check: https://stackoverflow.com/questions/59417433/typeerror-when-using-tf-keras-models-save-model-to-save-multi-inputs-tf-2-x-su\n",
        "    # And: https://stackoverflow.com/questions/62250441/saving-a-tensorflow-keras-model-encoder-decoder-to-savedmodel-format\n",
        "    # However, to date, there is no possible way to save and reload on TF2.5 this architecture\n",
        "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 1], dtype=tf.int32), \n",
        "                                  tf.TensorSpec(shape=[None, 64, 300], dtype=tf.float32),\n",
        "                                  tf.TensorSpec(shape=[None, 512], dtype=tf.float32)])\n",
        "    \n",
        "    def __call__(self, x, features, hidden):\n",
        "        # defining attention as a separate model\n",
        "        if self.baseline:\n",
        "            context_vector = self.no_attention(features)\n",
        "            attention_weights = None\n",
        "        else:\n",
        "            context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        if self.baseline:\n",
        "            x = tf.concat([tf.expand_dims(context_vector, 1), hidden, x], axis=-1)\n",
        "        else:\n",
        "            x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        if self.lstm_enabled:\n",
        "            # passing the concatenated vector to the LSTM\n",
        "            output, state, _ = self.lstm(x)\n",
        "        else:\n",
        "            # passing the concatenated vector to the GRU\n",
        "            output, state = self.gru(x)\n",
        "\n",
        "        # shape == (batch_size, max_length, hidden_size)\n",
        "        x = self.fc1(output)\n",
        "\n",
        "        # x shape == (batch_size * max_length, hidden_size)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size * max_length, vocab)\n",
        "        x = self.fc2(x)\n",
        "        #probs = tf.reshape(x, (-1, MAX_SEQ_LEN, self.vocab_size))\n",
        "        probs = tf.nn.softmax(x)\n",
        "\n",
        "        return x, state, attention_weights, probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bYN7xA0RPGl"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "@tf.function\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A3Ni64joyab"
      },
      "source": [
        "### Checkpoint\n",
        "\n",
        "Set checkpoints to save and load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpJAqPMWo0uE"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = f\"checkpoints/{DATASET_NAME}/train_{'lstm' if LSTM else 'gru'}_{EPOCHS}epochs{'_Baseline' if BASELINE else ''}\"\n",
        "print(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07pNWCyPHOoT"
      },
      "outputs": [],
      "source": [
        "encoder = CNN_Encoder(EMBEDDING_SIZE)\n",
        "decoder = RNN_Decoder(units, \n",
        "                      tf.convert_to_tensor(emb_matrix), \n",
        "                      len(val_to_key.keys()) + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHod7t72RPGn"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt4WZ5mhJE-E"
      },
      "outputs": [],
      "source": [
        "# adding this in a separate cell because if you run the training cell\n",
        "# many times, the loss_plot array will be reset\n",
        "loss_plot = []\n",
        "loss_plot_val = []\n",
        "\n",
        "loss_plot_by_steps = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqgyz2ANKlpU"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "\n",
        "    # initializing the hidden state for each batch\n",
        "    # because the captions are not related from image to image\n",
        "    hidden = tf.zeros((BATCH_SIZE, units))\n",
        "    dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(img_tensor)\n",
        "\n",
        "        for i in range(1, target.shape[1]):\n",
        "            # passing the features through the decoder\n",
        "            predictions, hidden, _, probs = decoder(dec_input, features, hidden)\n",
        "            loss += loss_function(target[:, i], predictions)\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    total_loss = (loss / int(target.shape[1]))\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    return loss, total_loss\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def val_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    # initializing the hidden state for each batch\n",
        "    # because the captions are not related from image to image\n",
        "    hidden = tf.zeros((BATCH_SIZE, units))\n",
        "    dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n",
        "    features = encoder(img_tensor)\n",
        "\n",
        "    for i in range(1, target.shape[1]):\n",
        "        # passing the features through the decoder\n",
        "        predictions, hidden, _, probs = decoder(dec_input, features, hidden)\n",
        "        loss += loss_function(target[:, i], predictions)\n",
        "        # using teacher forcing\n",
        "        dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    total_loss = (loss / int(target.shape[1]))\n",
        "    return loss, total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlA4VIQpRPGo"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(checkpoint_path):\n",
        "    os.mkdir(checkpoint_path)\n",
        "    progress_bar = tqdm(range(num_steps_per_epoch * EPOCHS))\n",
        "    print_str = f'Epoch {1} | Batch {0} | Loss ----- | '\n",
        "    progress_bar.set_description_str(print_str)\n",
        "\n",
        "    for epoch in range(0, EPOCHS):\n",
        "        start = time.time()\n",
        "        total_loss = 0\n",
        "        total_loss_val = 0\n",
        "\n",
        "        for (batch, (img_tensor, target)) in enumerate(dataset_generator(img_name_train, cap_train)):\n",
        "            batch_loss, t_loss = train_step(img_tensor, target)\n",
        "            total_loss += t_loss\n",
        "            loss_plot_by_steps.append(t_loss)\n",
        "\n",
        "            average_batch_loss = batch_loss.numpy() / int(target.shape[1])\n",
        "            if batch % 5 == 0:\n",
        "                print_str = f'Epoch {epoch + 1} | Batch {batch + 5} | Loss {average_batch_loss:.4F} | '\n",
        "                progress_bar.set_description_str(print_str)\n",
        "                progress_bar.update(5)\n",
        "                \n",
        "        for (batch, (img_tensor, target)) in enumerate(dataset_generator(img_name_val, cap_val)):\n",
        "            batch_loss, t_loss = val_step(img_tensor, target)\n",
        "            total_loss_val += t_loss\n",
        "\n",
        "        # storing the epoch end loss value to plot later\n",
        "        loss_plot.append(total_loss / num_steps_per_epoch)\n",
        "        loss_plot_val.append(total_loss_val / num_steps_val_per_epoch)\n",
        "        \n",
        "        print()\n",
        "        print(f'Epoch {epoch + 1} Loss {total_loss / num_steps_per_epoch:.4F} Val Loss {total_loss_val / num_steps_val_per_epoch:.4F}')\n",
        "        print(f'Time taken for 1 epoch {time.time() - start:.2f} sec\\n')\n",
        "        \n",
        "    plt.plot(loss_plot)\n",
        "    plt.plot(loss_plot_val)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Plot')\n",
        "    plt.legend(['train', 'validation'])\n",
        "    plt.savefig(f\"{checkpoint_path}/loss.png\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(loss_plot_by_steps)\n",
        "    plt.xlabel('Num Steps')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Plot by Steps')\n",
        "    plt.legend(['train'])\n",
        "    plt.savefig(f\"{checkpoint_path}/loss_plot_by_steps.png\")\n",
        "    plt.show()\n",
        "\n",
        "    tf.saved_model.save(encoder, f\"{checkpoint_path}/encoder\")\n",
        "    tf.saved_model.save(decoder, f\"{checkpoint_path}/decoder\")     \n",
        "    #     encoder.save_weights(f\"{checkpoint_path}/encoder\")\n",
        "    #     decoder.save_weights(f\"{checkpoint_path}/decoder\")     \n",
        "    \n",
        "else:\n",
        "    image = Image.open(f\"{checkpoint_path}/loss.png\")\n",
        "    image.show()\n",
        "    image = Image.open(f\"{checkpoint_path}/loss_plot_by_steps.png\")\n",
        "    image.show()\n",
        "    \n",
        "    encoder = tf.saved_model.load(f\"{checkpoint_path}/encoder\")\n",
        "    decoder = tf.saved_model.load(f\"{checkpoint_path}/decoder\")\n",
        "    #     encoder.load_weights(f\"{checkpoint_path}/encoder\")\n",
        "    #     decoder.load_weights(f\"{checkpoint_path}/decoder\")   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGvOcLQKghXN"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "* The evaluate function is similar to the training loop, except you don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the end token.\n",
        "* And store the attention weights for every time step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f1CUBUFOc60"
      },
      "outputs": [],
      "source": [
        "def predict(sent_predict, features, hidden):\n",
        "    '''\n",
        "    ::params\n",
        "    src_input: sequence generated at this point\n",
        "    features: image features extracted\n",
        "    return : hidden (context) and new probilites from softmax\n",
        "    '''\n",
        "    predictions, hidden, attention_weights, probs = decoder(sent_predict,\n",
        "                                                            features,\n",
        "                                                            hidden)\n",
        "    return hidden, probs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFQaDICz9cSf"
      },
      "outputs": [],
      "source": [
        "def beam_search(features, beam_size=2, top_k=2, sequence_max_len=MAX_SEQ_LEN):\n",
        "    # (probability, token sequence, last token)\n",
        "    # first element: incremental probability of the sentence until the current point\n",
        "    # second element: token sequence corresponding to the generated sentence\n",
        "    # third element: next possible token\n",
        "    k_beams_running = [(0, [word_to_index('<start>')], [word_to_index('<start>')]) for i in range(beam_size)]\n",
        "\n",
        "    # initialize list to store completed sentences\n",
        "    k_beams_ended = []\n",
        "\n",
        "    # initialize hiddens for each beam\n",
        "    hiddens = [tf.zeros((1, units)) for _ in range(beam_size)]\n",
        "\n",
        "    first = True\n",
        "    while beam_size > 0:\n",
        "        # list of beams: contains all the possible sentences obtained by concatenating current sentence with the\n",
        "        # next possible word\n",
        "        list_of_beams = []\n",
        "        for beam in range(beam_size):\n",
        "            hiddens[beam], probs = predict(tf.expand_dims((k_beams_running[beam][2]), 0), features, hiddens[beam])\n",
        "            # extract top_k probabilites and relative indices\n",
        "            predicted = tf.math.top_k(tf.math.log(probs), k=top_k)\n",
        "            top_k_indices = predicted.indices.numpy()\n",
        "            top_k_logprobs = predicted.values.numpy()\n",
        "            # adding temporary sums for logprobs\n",
        "            for i in range(top_k):\n",
        "                k_beams_running[beam][1].append(top_k_indices[i])\n",
        "                running_copy = copy.deepcopy(k_beams_running[beam])\n",
        "                list_of_beams.append((running_copy[0] + top_k_logprobs[i], running_copy[1], [top_k_indices[i]]))\n",
        "                k_beams_running[beam][1].pop()\n",
        "            # if first time just have top_k possible paths instead of beam_size * top_k\n",
        "            if first:\n",
        "                first = False\n",
        "                break\n",
        "        beams_to_remove = []\n",
        "        for beam in range(beam_size):\n",
        "            # extract tuple corresponding to max probability\n",
        "            max_s = max(list_of_beams, key=itemgetter(0))\n",
        "            k_beams_running[beam] = max_s\n",
        "            list_of_beams.remove(max_s)\n",
        "            # if token correspond to <end> or exceedes max length store in k_beams_ended\n",
        "            if k_beams_running[beam][2][0] == word_to_index('<end>') or len(\n",
        "                    k_beams_running[beam][1]) >= sequence_max_len:\n",
        "                k_beams_ended.append((k_beams_running[beam][0], k_beams_running[beam][1]))\n",
        "                beams_to_remove.append(beam)\n",
        "                beam_size -= 1\n",
        "        for i, beam in enumerate(beams_to_remove):\n",
        "            # remove from k_beams running the ended sentence\n",
        "            k_beams_running.pop(beam - i)\n",
        "            hiddens.pop(beam - i)\n",
        "\n",
        "    # normalize\n",
        "    normalize_probs = []\n",
        "    for sen in k_beams_ended:\n",
        "        normalize_probs.append(sen[0] / len(sen[1]))\n",
        "    # extract sentence with maximum probability\n",
        "    tokenized_sen = k_beams_ended[np.argmax(normalize_probs)][1]\n",
        "    sen = [index_to_word(tokenized_word) for tokenized_word in tokenized_sen]\n",
        "    return sen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isRsJ-VuO4t_"
      },
      "outputs": [],
      "source": [
        "def beam_search_evaluate(image_features):\n",
        "    features = encoder(image_features)\n",
        "    return beam_search(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCWpDtyNRPGs"
      },
      "outputs": [],
      "source": [
        "def evaluate(image_features):\n",
        "    attention_plot = np.zeros((MAX_SEQ_LEN, attention_features_shape))\n",
        "    hidden = tf.zeros((1, units))\n",
        "    features = encoder(image_features)\n",
        "    dec_input = tf.expand_dims([word_to_index('<start>')], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(MAX_SEQ_LEN):\n",
        "        predictions, hidden, attention_weights, probs = decoder(dec_input,\n",
        "                                                                features,\n",
        "                                                                hidden)\n",
        "\n",
        "        if not BASELINE:\n",
        "            attention_plot[i] = tf.reshape(attention_weights, (-1,)).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        predicted_word = tf.compat.as_text(index_to_word(predicted_id))\n",
        "        result.append(predicted_word)\n",
        "\n",
        "        if predicted_word == '<end>':\n",
        "            if not BASELINE:\n",
        "                return result, attention_plot\n",
        "            else:\n",
        "                return result\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    if not BASELINE:\n",
        "        attention_plot = attention_plot[:len(result), :]\n",
        "        return result, attention_plot\n",
        "    else:\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD_y7PD6RPGt"
      },
      "outputs": [],
      "source": [
        "def plot_attention(image, result, attention_plot):\n",
        "    fig = plt.figure(figsize=(18, 18))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for i in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
        "        grid_size = max(int(np.ceil(len_result / 2)), 2)\n",
        "        ax = fig.add_subplot(grid_size, grid_size, i + 1)\n",
        "        ax.set_title(result[i], fontsize=19)\n",
        "        img = ax.imshow(image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.3, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHUvtPhACO-i"
      },
      "outputs": [],
      "source": [
        "# captions on the validation set\n",
        "\n",
        "def print_one_output(rid, image, cap_split):\n",
        "    print(image.shape)\n",
        "    image_features = forward_in_inception(np.array([image], dtype=np.float32))\n",
        "    print(image_features.shape)\n",
        "\n",
        "    real_caption = ' '.join([tf.compat.as_text(index_to_word(i)) for i in cap_split[rid] if i not in [0]])\n",
        "    if BASELINE:\n",
        "        result = evaluate(image_features)\n",
        "    else:\n",
        "        result, attention_plot = evaluate(image_features)\n",
        "    print()\n",
        "    print('Real Caption::\\t\\t', real_caption[8:-6])\n",
        "    print('Prediction Caption::\\t', ' '.join(result[:-1]))\n",
        "    print('Beam search output::\\t', ' '.join(beam_search_evaluate(image_features)[1:-1]))\n",
        "\n",
        "    if not BASELINE:\n",
        "        plot_attention(image, result, attention_plot)\n",
        "\n",
        "    print()\n",
        "    \n",
        "rid = np.random.randint(len(img_name_val))        \n",
        "image = tf.io.decode_jpeg(dataset['input_image'][img_name_val[rid]])\n",
        "\n",
        "print_one_output(rid, image, cap_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a9toaVyHOoY"
      },
      "source": [
        " ## Error Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjORDcpvMYNQ"
      },
      "outputs": [],
      "source": [
        "# BEST, WORST, AND AVG, CAPTIONS based on Bleu scores\n",
        "\n",
        "bleu_argmax = []\n",
        "bleu_beam = []\n",
        "\n",
        "y_argmax = []\n",
        "y_beam = []\n",
        "y_true = []\n",
        "\n",
        "for el_id in tqdm(range(len(img_name_test[:100]))):\n",
        "    image = tf.io.decode_jpeg(dataset['input_image'][img_name_test[el_id]])\n",
        "    image_features = forward_in_inception(np.array([image], dtype=np.float32))\n",
        "\n",
        "    real_caption = ' '.join([tf.compat.as_text(index_to_word(j)) for j in cap_test[el_id] if j not in [0]])\n",
        "    real_caption = re.sub(r'(<start> )|( <end>)', '', real_caption)\n",
        "    beam = ' '.join(beam_search_evaluate(image_features)[1:-1])\n",
        "    if BASELINE:\n",
        "        argmax = ' '.join(evaluate(image_features)[:-1])\n",
        "    else:\n",
        "        argmax = ' '.join(evaluate(image_features)[0][:-1])\n",
        "\n",
        "    try:\n",
        "        bleu_beam.append(sentence_bleu([real_caption], beam, smoothing_function=SmoothingFunction().method4))\n",
        "        bleu_argmax.append(sentence_bleu([real_caption], argmax, smoothing_function=SmoothingFunction().method4))\n",
        "\n",
        "        y_argmax.append(argmax)\n",
        "        y_beam.append(beam)\n",
        "        y_true.append(real_caption)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(real_caption)\n",
        "\n",
        "        \n",
        "# BEST\n",
        "print(f\"Max Bleu {max(bleu_argmax)}\")\n",
        "index = np.argmax(bleu_argmax)\n",
        "image = tf.io.decode_jpeg(dataset['input_image'][img_name_test[index]])\n",
        "print_one_output(index, image, cap_test)\n",
        "\n",
        "# WORST\n",
        "print(f\"Min Bleu {min(bleu_argmax)}\")\n",
        "index = np.argmin(bleu_argmax)\n",
        "image = tf.io.decode_jpeg(dataset['input_image'][img_name_test[index]])\n",
        "print_one_output(index, image, cap_test)\n",
        "\n",
        "# AVERAGE\n",
        "average = np.mean(bleu_argmax)\n",
        "print(f\"Average Bleu {average}\")\n",
        "index = np.argmin([np.abs(average - s) for s in bleu_argmax])\n",
        "image = tf.io.decode_jpeg(dataset['input_image'][img_name_test[index]])\n",
        "print_one_output(index, image, cap_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSwtgrxPHOoZ"
      },
      "outputs": [],
      "source": [
        "def print_element_from_caption(caption):\n",
        "    caption = re.sub(r'^', '<start> ', caption)\n",
        "    caption = re.sub(r'$', ' <end>', caption)\n",
        "    indexes = [i for i, x in enumerate(cleaned_captions_maxthr) if x == caption]\n",
        "    print(len(indexes))\n",
        "    if len(indexes) > 0:\n",
        "        for index in indexes:\n",
        "            image = tf.io.decode_jpeg(dataset['input_image'][img_indexes_maxthr[index]])\n",
        "            image_features = forward_in_inception(np.array([image], dtype=np.float32))\n",
        "\n",
        "            real_caption = re.sub(r'(<start> )|( <end>)', '', caption)\n",
        "            if BASELINE:\n",
        "                argmax = ' '.join(evaluate(image_features)[:-1])\n",
        "            else:\n",
        "                argmax = ' '.join(evaluate(image_features)[0][:-1])\n",
        "\n",
        "            print('Real Caption::\\t\\t', real_caption)\n",
        "            print('Prediction Caption::\\t', argmax)\n",
        "\n",
        "            plt.figure(figsize=(6,6))\n",
        "            plt.imshow(image.numpy())\n",
        "            plt.show()\n",
        "    else:\n",
        "        print(\"Caption inserted is NOT present in the dataset\")\n",
        "    \n",
        "print_element_from_caption(\"the lady is wearing a western multicolor long sleeved dress\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOTh3KdlHOoa"
      },
      "source": [
        "## Evaluation with BLEU, CHRF and BERTscore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KruumgnbhZMq"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "y_pred = []\n",
        "y_pred_nobeam = []\n",
        "y_true = []\n",
        "\n",
        "chrf_scores = []\n",
        "bleu_scores = []\n",
        "meteor_scores = []\n",
        "chrf_scores_nobeam = []\n",
        "bleu_scores_nobeam = []\n",
        "meteor_scores_nobeam = []\n",
        "\n",
        "\n",
        "for i in tqdm(range(len(img_name_test))):\n",
        "    decoded_img = np.array([tf.io.decode_jpeg(dataset['input_image'][img_name_test[i]])], dtype=np.float32)\n",
        "    image = forward_in_inception(decoded_img)\n",
        "    real_caption = ' '.join([tf.compat.as_text(index_to_word(w)) for w in cap_test[i] if w not in [0]])\n",
        "    real_caption = re.sub(r'(<start> )|( <end>)', '', real_caption)\n",
        "    output = ' '.join(beam_search_evaluate(image)[1:-1])\n",
        "    if BASELINE:\n",
        "        output_nobeam = ' '.join(evaluate(image)[:-1])\n",
        "    else:   \n",
        "        output_nobeam = ' '.join(evaluate(image)[0][:-1])\n",
        "\n",
        "    try:\n",
        "        chrf_scores.append(sentence_chrf(real_caption, output))\n",
        "        bleu_scores.append(sentence_bleu([real_caption], output, smoothing_function=SmoothingFunction().method4))\n",
        "        meteor_scores.append(meteor_score([real_caption.split()], output.split()))\n",
        "\n",
        "        chrf_scores_nobeam.append(sentence_chrf(real_caption, output_nobeam))\n",
        "        bleu_scores_nobeam.append(sentence_bleu([real_caption], output_nobeam, smoothing_function=SmoothingFunction().method4))\n",
        "        meteor_scores_nobeam.append(meteor_score([real_caption.split()], output_nobeam.split()))\n",
        "\n",
        "        y_pred.append(output)\n",
        "        y_pred_nobeam.append(output_nobeam)\n",
        "        y_true.append(real_caption)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(real_caption)\n",
        "\n",
        "print(\"BEAM:\")\n",
        "print(y_pred[1:5])\n",
        "\n",
        "print(\"NO BEAM:\")\n",
        "print(y_pred_nobeam[1:5])\n",
        "\n",
        "print(\"TRUE:\")\n",
        "print(y_true[1:5])\n",
        "\n",
        "print()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhHaw27iAOa7"
      },
      "outputs": [],
      "source": [
        "# You might run out of GPU memory to use bert\n",
        "# Only way to date to clear in a concise way nvidia gpu memory with tensorflow models\n",
        "\n",
        "device = cuda.get_current_device()\n",
        "device.reset()\n",
        "del encoder\n",
        "del decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL0xn6OYAOa7"
      },
      "outputs": [],
      "source": [
        "bleu_scr = sum(bleu_scores) / len(bleu_scores)\n",
        "chrf_scr = sum(chrf_scores) / len(chrf_scores)\n",
        "meteor_scr = sum(meteor_scores) / len(meteor_scores)\n",
        "bert_scr_prec, bert_scr_recall, bert_scr_f1 = bert_score(y_pred, y_true, lang=\"en\", verbose=False)\n",
        "bleu_scr_nobeam = sum(bleu_scores_nobeam) / len(bleu_scores_nobeam)\n",
        "chrf_scr_nobeam = sum(chrf_scores_nobeam) / len(chrf_scores_nobeam)\n",
        "meteor_scr_nobeam = sum(meteor_scores_nobeam) / len(meteor_scores_nobeam)\n",
        "bert_scr_prec_nobeam, bert_scr_recall_nobeam, bert_scr_f1_nobeam = bert_score(y_pred_nobeam, y_true, lang=\"en\",\n",
        "                                                                              verbose=False)\n",
        "\n",
        "print()\n",
        "print(f\"BLEU beam: {bleu_scr:.3F} \"\n",
        "      f\"CHRF beam: {chrf_scr:.3F} \"\n",
        "      f\"METEOR beam: {meteor_scr:.3F} \"\n",
        "      f\"BERTscore beam: {bert_scr_f1.mean():.3F} \")\n",
        "print(f\"BLEU: {bleu_scr_nobeam:.3F} \"\n",
        "      f\"CHRF: {chrf_scr_nobeam:.3F} \"\n",
        "      f\"METEOR: {meteor_scr_nobeam:.3F} \"\n",
        "      f\"BERTscore: {bert_scr_f1_nobeam.mean():.3F}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWDHHOyyIKPB"
      },
      "outputs": [],
      "source": [
        "def plot_scores(labels: list, values: list, title: str):\n",
        "    x = np.array(labels)\n",
        "    y = np.array([round(val, 3) for val in values])\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 5))\n",
        "    ax = fig.add_subplot(111)\n",
        "    yvals = range(len(y))\n",
        "\n",
        "    ax.barh(yvals, y, align='center', alpha=0.4, color=['red', 'blue', 'yellow', 'green'])\n",
        "    plt.yticks(yvals, x)\n",
        "    plt.xlim([0, 1.1])\n",
        "    plt.axvline(x = 1, color='black', label='axvline - full height', lw=0.3)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    [ax.bar_label(container, fontsize=10, rotation='60') for container in ax.containers]\n",
        "    plt.savefig(f'plots/{title}.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHtUAJjJAOa7"
      },
      "outputs": [],
      "source": [
        "values = [bleu_scr, chrf_scr, meteor_scr, bert_scr_f1.mean().item()]\n",
        "labels = [\"BLEU\", \"CHRF\", \"METEOR\", \"BERT\"]\n",
        "\n",
        "plot_scores(labels, values, f\"Scores with BEAM search on {DATASET_NAME} {EPOCHS}epochs \"\n",
        "                            f\"{'LSTM' if LSTM else 'GRU'} {EMBEDDING_SIZE}emb_size \" \n",
        "                            f\"{'Baseline' if BASELINE else ''}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS4QjXAhAOa7"
      },
      "outputs": [],
      "source": [
        "values = [bleu_scr_nobeam, chrf_scr_nobeam, meteor_scr_nobeam, bert_scr_prec_nobeam.mean().item()]\n",
        "labels = [\"BLEU\", \"CHRF\", \"METEOR\", \"BERT\"]\n",
        "\n",
        "plot_scores(labels, values, f\"Scores with ARGMAX generation on {DATASET_NAME} {EPOCHS}epochs \"\n",
        "                            f\"{'LSTM' if LSTM else 'GRU'} {EMBEDDING_SIZE}emb_size \" \n",
        "                            f\"{'Baseline' if BASELINE else ''}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Visionizer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}