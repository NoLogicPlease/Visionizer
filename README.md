# Visionizer

<p align="center">
  <img width="600" src="images/logo.png">
</p>

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7196078.svg)](https://doi.org/10.5281/zenodo.7196078)

Fashion e-commerce platforms are becoming increasingly popular. However, scanning, 1
rendering, and captioning fashion items is still done mostly manually. In this work, we address 2
fashion image captioning, i.e., the task of generating a textual description of a fashion item from an 3
image portraying it. We curate a novel dataset for such task, Fashion-Cap, which contains more than 4
290,000 images and 40,000 corresponding captions. We run an extensive study with several neural 5
architectures over 3 fashion captioning datasets including ours. 

## Attention Weights during generation
<p align="center">
  <img width="600" src="images/infashai_lstm.png">
</p>

 
## Datasets

The following datasets were used, and are available here: [https://doi.org/10.5281/zenodo.7196078](https://doi.org/10.5281/zenodo.7196078)
* FashionCap
* ReducedFACAD
* ReducedInFashAI
